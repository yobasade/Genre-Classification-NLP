{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Modeling\n",
    "\n",
    "This notebook will focus on exploring our lyrics dataset and creating our model for our lyrics classifier. Analysis on model performance and expectations will be present throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import nltk\n",
    "import string\n",
    "import ast \n",
    "import gensim\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.stem import PorterStemmer\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook, reset_output\n",
    "from bokeh.palettes import d3\n",
    "import bokeh.models as bmo\n",
    "from bokeh.io import save, output_file\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading our Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vec = Word2Vec.load(r'C:\\Users\\Fib0nacci\\Desktop\\ML_genres.model') #Reloading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = Word2Vec.load(r'C:\\Users\\Fib0nacci\\Desktop\\data\\model.bin') #reloading our bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our vector on the word \"baby\"\n",
    "print(model_vec.wv.vocab[\"baby\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the size of our vector for one word \"baby\". We want to see what this looks like.\n",
    "print(len(model_vec.wv['baby']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of words.\n",
    "print(len(model_vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets repeat this process for a few more words that we saw present in our top 10s above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    " #This function will print the word index and vector length generated for each input.\n",
    "    def word_info(word):\n",
    "        print(f\"Index of the word {word}:\")\n",
    "        print(model_vec.wv.vocab[word].index)\n",
    "        print(\"Length of the vector generated for a word\")\n",
    "        print(len(model_vec.wv[word]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_info('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets pull out some words that have strong correlations and look at their similarity value. These words will be opposites of each other to start, but I also want to look at words that are closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w1, w2):\n",
    "    print(model_vec.wv.similarity(w1=w1, w2=w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('happy', 'sad') #opposites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vec.wv.most_similar('love') #The scores for words similar to 'love' are generally in the higher range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model_bin.wv.vocab) #The list of words in the word 2 vec bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time() #instantiating a timer to view how long it takes for our model to train on our lyrics data.\n",
    "\n",
    "model_vec.train(lyrics_df['lyrics_cleaned'], total_examples=model_vec.corpus_count,epochs=50) #Adding a value of 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Word2vec Embeddings\n",
    "\n",
    "I will use plotly to visualize the words in our word2vec corpus. This will aloow me to analyize the positions and relationships amongst my word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A portion of this code was adapted from Eric Klepplen on towardsdatascience.com\n",
    "X = model_vec[model_bin.wv.vocab] #Implementing PCA and fitting our results.\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "res = pca.fit_transform(X)\n",
    "Creating a dataframe\n",
    "pca_df = pd.DataFrame(res, columns = ['x', 'y'])\n",
    "\n",
    "pca_df['words'] = words\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A portion of this code was adapted from Eric Klepplen on towardsdatascience.com\n",
    "N = 1000000 #number of randomizations\n",
    "words = list(model_bin.wv.vocab)\n",
    "fig = go.Figure(data=go.Scattergl( #Using go to visualize our scatterplot\n",
    "    x = pca_df['x'],\n",
    "    y = pca_df['y'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=np.random.randn(N), #Our marker is a dictionary of the colors\n",
    "        colorscale='plasma',\n",
    "        line_width=1\n",
    "    ),\n",
    "    text=pca_df['words'],\n",
    "    textposition=\"bottom center\"\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #This is a progress bar to help us see where our vectorizor is in transforming our X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A portion of this code was adapted from stackoverflow.com/\n",
    "def vectorized(X_train):\n",
    "    vector=[]\n",
    "    for sent in tqdm(X_train['lyrics_cleaned'].values):\n",
    "        sent_vec=np.zeros(200)\n",
    "        count =0\n",
    "        for word in sent: \n",
    "            if word in w2v_words:\n",
    "                vec = model.wv[word]\n",
    "                sent_vec += vec \n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            sent_vec /= count #normalize\n",
    "        vector.append(sent_vec)\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec \n",
    "\n",
    "I will now go further by using doc2vec to create a vectorized representation of word groups taken collectively. I want to do this so we can better understand the relationships among our lyrics and our genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will begin by implementing a function to tag our document.\n",
    "def tagged_doc(ls_of_ls_of_words):\n",
    "    for i, list_of_words in enumerate(ls_of_ls_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_doc(lyrics_df['lyrics_cleaned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a vector size of 50 as suggested by the documentation at *https://radimrehurek.com/gensim/*. I chose an epoch size of 10, since I have produced thousands of docs. Since my dataset is large, I want to reduce my iterations. I chose a minimum word count of 2 to discard words that have few occurances. this was also a suggestion of the documentation source stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc.build_vocab(data_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc.train(data_for_training, total_examples=model_doc.corpus_count, epochs=model_doc.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc.save(r'C:\\Users\\Fib0nacci\\Desktop\\data\\doc2vec.model') #Saving our vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "Part 2 using Doc2Vec and Visualization is in the second notebook named: Lyrics_Data_Modeling_Part_2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
